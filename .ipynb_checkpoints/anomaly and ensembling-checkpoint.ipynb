{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c214ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "from random import sample\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import scipy.stats as stats\n",
    "from random import choices\n",
    "import statsmodels.api as sm\n",
    "import pylab as py\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import expon\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5543f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each domain has a label with at most 63 characters that consists only of English letters, digits, or hyphens\n",
    "def clean_data(domains, labels):\n",
    "    \n",
    "    index = 0\n",
    "    for domain in domains:\n",
    "        if len(domain) > 63 or not re.match(\"^[A-Za-z0-9._-]*$\", domain):\n",
    "            domains.remove(domain)\n",
    "            labels.remove(labels[index])\n",
    "            index -= 1\n",
    "        index += 1\n",
    "        \n",
    "    return domains, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d7307dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in original test and training data\n",
    "\n",
    "with open('test1.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    test1 = [line.rstrip() for line in lines]\n",
    "    \n",
    "with open('test2.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    test2 = [line.rstrip() for line in lines]\n",
    "    \n",
    "with open('test1label.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    test1label = [line.rstrip() for line in lines]\n",
    "    \n",
    "with open('test2label.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    test2label = [line.rstrip() for line in lines]\n",
    "    \n",
    "with open('training.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    train = []\n",
    "    trainlabel = []\n",
    "    for row in reader:\n",
    "        train.append(row[0])\n",
    "        trainlabel.append(row[1])\n",
    "        \n",
    "#clean data\n",
    "#test1, test1label = clean_data(test1, test1label)\n",
    "#test2, test2label = clean_data(test2, test2label)\n",
    "#train, trainlabel = clean_data(train, trainlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a5102a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort original training data into benign and malicious\n",
    "newtrainAGD = []\n",
    "newtrainbenign = []\n",
    "for i in range(len(trainlabel)):\n",
    "    if trainlabel[i] == '1':\n",
    "        newtrainAGD.append(train[i])\n",
    "    else:\n",
    "        newtrainbenign.append(train[i])\n",
    "newAGDlabel = ['1']*len(newtrainAGD)\n",
    "newbenignlabel = ['0']*len(newtrainbenign)\n",
    "\n",
    "#Sort original testing data into benign and malicious\n",
    "newtestAGD = []\n",
    "newtestbenign = []\n",
    "for i in range(len(test1label)):\n",
    "    if test1label[i] == '1':\n",
    "        newtestAGD.append(test1[i])\n",
    "    else:\n",
    "        newtestbenign.append(test1[i])\n",
    "\n",
    "for i in range(len(test2label)):\n",
    "    if test2label[i] == '1':\n",
    "        newtestAGD.append(test2[i])\n",
    "    else:\n",
    "        newtestbenign.append(test2[i])\n",
    "        \n",
    "newtestAGDlabel = ['1']*len(newtestAGD)\n",
    "newtestbenignlabel = ['0']*len(newtestbenign)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80308a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in MaskDGA adversarial training and test datasets\n",
    "with open('advTrain_sub-cnn_prec-50.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    advTrain = []\n",
    "    for row in reader:\n",
    "        advTrain.append(row[2])\n",
    "    advTrain.remove('url')\n",
    "    \n",
    "with open('test_sub-cnn_prec-25.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    advTest25 = []\n",
    "    for row in reader:\n",
    "        advTest25.append(row[2])\n",
    "    advTest25.remove('url')\n",
    "    \n",
    "with open('test_sub-cnn_prec-50.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    advTest50 = []\n",
    "    for row in reader:\n",
    "        advTest50.append(row[2])\n",
    "    advTest50.remove('url')\n",
    "    \n",
    "with open('test_sub-cnn_prec-75.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    advTest75 = []\n",
    "    for row in reader:\n",
    "        advTest75.append(row[2])\n",
    "    advTest75.remove('url')\n",
    "    \n",
    "with open('test_sub-lstm_prec-50.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    advTestlstm50 = []\n",
    "    for row in reader:\n",
    "        advTestlstm50.append(row[2])\n",
    "    advTestlstm50.remove('url')\n",
    "        \n",
    "with open('test_sub-lstm_prec-75.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    advTestlstm75 = []\n",
    "    for row in reader:\n",
    "        advTestlstm75.append(row[2])\n",
    "    advTestlstm75.remove('url')\n",
    "    \n",
    "advTrainlabel = ['2']*len(advTrain)\n",
    "advTestlabel25 = ['2']*len(advTest25)\n",
    "advTestlabel50 = ['2']*len(advTest50)\n",
    "advTestlabel75 = ['2']*len(advTest75)\n",
    "advTest2label50 = ['2']*len(advTestlstm50)\n",
    "advTest2label75 = ['2']*len(advTestlstm75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c6c2780",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract features from domain names\n",
    "#length of domain name, percentage of numerical characters, percentage of vowels, percentage of symbols,\n",
    "#flag to see if first character is a digit, flag to see if name contains alphanumeric characters\n",
    "#consecutive consonant's percentage\n",
    "\n",
    "def extract(domains):\n",
    "    \n",
    "    #features\n",
    "    length = []\n",
    "    num_perc = []\n",
    "    vowel_perc = []\n",
    "    symbol_perc = []\n",
    "    first_digit = []\n",
    "    mixture = []\n",
    "    consec = []\n",
    "    \n",
    "    for name in domains:\n",
    "        \n",
    "        name_length = len(name)\n",
    "        length.append(name_length)\n",
    "        #length.append(0)         #toggles off length being a feature\n",
    "        \n",
    "        numer = 0\n",
    "        vowel = 0\n",
    "        symbol = 0\n",
    "        counter = 0\n",
    "        has_letter = False\n",
    "        has_number = False\n",
    "        last_cons = False\n",
    "        for ch in name:\n",
    "            if ch.isdigit():\n",
    "                numer += 1\n",
    "                has_number = True\n",
    "            elif ch in ['a','e','i','o','u']:\n",
    "                vowel += 1\n",
    "            elif ch in ['.','-','_']:\n",
    "                symbol += 1\n",
    "                \n",
    "            if ch.isalpha():\n",
    "                has_letter = True\n",
    "                \n",
    "            if ch not in ['a','e','i','o','u']:\n",
    "                if last_cons == True:\n",
    "                    counter += 1\n",
    "                last_cons = True\n",
    "            else:\n",
    "                last_cons = False\n",
    "                \n",
    "        if has_letter and has_number:\n",
    "            mixture.append(1)\n",
    "            #mixture.append(0)\n",
    "        else:\n",
    "            mixture.append(0)\n",
    "        \n",
    "        vowel_perc.append(vowel/name_length)\n",
    "        num_perc.append(numer/name_length)\n",
    "        symbol_perc.append(symbol/name_length)\n",
    "        consec.append(counter/(name_length))\n",
    "        #consec.append(0)\n",
    "        \n",
    "        if name[0].isdigit():\n",
    "            first_digit.append(1)\n",
    "        else:\n",
    "            first_digit.append(0)\n",
    "\n",
    "    features = [length, num_perc, vowel_perc, symbol_perc, first_digit, mixture, consec]\n",
    "    \n",
    "    return features\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f461e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts to form suitable to input to python function\n",
    "def convert(features):\n",
    "    \n",
    "    length = features[0]\n",
    "    num_perc = features[1]\n",
    "    vowel_perc = features[2]\n",
    "    symbol_perc = features[3]\n",
    "    first_digit = features[4]\n",
    "    mixture = features[5]\n",
    "    consec = features[6]\n",
    "    num_domains = len(length)\n",
    "    \n",
    "    X = [0]*num_domains\n",
    "    for i in range(num_domains):\n",
    "        X[i] = [length[i], num_perc[i], vowel_perc[i], symbol_perc[i], first_digit[i], mixture[i], consec[i]]\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb59450",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "split=split1=split2= 20000\n",
    "\n",
    "#Average features for the 3 types of domains\n",
    "examine_adv = extract(advTrain[0:split])\n",
    "examine_benign = extract(newtrainbenign[0:split1])\n",
    "examine_AGD = extract(newtrainAGD[0:split2])\n",
    "\n",
    "#Length\n",
    "print(\"LENGTH\")\n",
    "print(np.mean(examine_adv[0]), np.var(examine_adv[0]))\n",
    "print(np.mean(examine_benign[0]), np.var(examine_benign[0]))\n",
    "print(np.mean(examine_AGD[0]), np.var(examine_AGD[0]))\n",
    "\n",
    "#Digit ratio\n",
    "print(\"\\nDigit Ratio\")\n",
    "print(np.mean(examine_adv[1]), np.var(examine_adv[1]))\n",
    "print(np.mean(examine_benign[1]), np.var(examine_benign[1]))\n",
    "print(np.mean(examine_AGD[1]), np.var(examine_AGD[1]))\n",
    "\n",
    "#Vowel ratio\n",
    "print(\"\\nVowel Ratio\")\n",
    "print(np.mean(examine_adv[2]), np.var(examine_adv[2]))\n",
    "print(np.mean(examine_benign[2]), np.var(examine_benign[2]))\n",
    "print(np.mean(examine_AGD[2]), np.var(examine_AGD[2]))\n",
    "\n",
    "#Symbol ratio\n",
    "print(\"\\nSymbol Ratio\")\n",
    "print(np.mean(examine_adv[3]), np.var(examine_adv[3]))\n",
    "print(np.mean(examine_benign[3]), np.var(examine_benign[3]))\n",
    "print(np.mean(examine_AGD[3]), np.var(examine_AGD[3]))\n",
    "\n",
    "#Flag for first digit\n",
    "print(\"\\nFirst Digit\")\n",
    "print(np.mean(examine_adv[4]))\n",
    "print(np.mean(examine_benign[4]))\n",
    "print(np.mean(examine_AGD[4]))\n",
    "\n",
    "#Flag for mixture\n",
    "print(\"\\nMixture\")\n",
    "print(np.mean(examine_adv[5]))\n",
    "print(np.mean(examine_benign[5]))\n",
    "print(np.mean(examine_AGD[5]))\n",
    "\n",
    "#Consecutive consonant ratio\n",
    "print(\"\\nConsecutive Consonant Ratio\")\n",
    "print(np.mean(examine_adv[6]), np.var(examine_adv[6]))\n",
    "print(np.mean(examine_benign[6]), np.var(examine_benign[6]))\n",
    "print(np.mean(examine_AGD[6]), np.var(examine_AGD[6]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b03f5621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the distribution of each continuous feature given class (index 1,2,3,6)\n",
    "#digit ratio, vowel ratio, symbol ratio, consecutive consonant ratio\n",
    "  \n",
    "# Random data points generated\n",
    "#data_points = np.array(examine_adv[6])\n",
    "#data_points = np.array(examine_benign[6])\n",
    "data_points = np.array(examine_AGD[6])\n",
    "\n",
    "#plot normal distribution\n",
    "norm_points = np.random.normal(np.mean(data_points), np.sqrt(np.var(data_points)), 20000)  \n",
    "mu, std = norm.fit(norm_points)\n",
    "plt.hist(data_points, bins = 10, density=True)\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 1000)\n",
    "p = norm.pdf(x, mu, std)\n",
    "plt.title('Digit Ratio')\n",
    "plt.title('Vowel Ratio')\n",
    "plt.title('Symbol Ratio')\n",
    "plt.title('Consecutive Consonant Ratio')\n",
    "plt.xlabel('feature value')\n",
    "plt.ylabel('frequency')\n",
    "plt.plot(x,p)\n",
    "plt.show()\n",
    "fig = plt.figure(figsize =(8, 5))\n",
    "plt.boxplot(data_points)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14a20088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#common function used by filter_density and filter_relative_density\n",
    "#computes the average density of a point\n",
    "def average_density(j, length, sorted_list, k):\n",
    "    \n",
    "    #Takes the 2 values above and two values below in the sorted list, and sum their distances from the current feature\n",
    "    if j >= k and j < length-k:\n",
    "        n1 = j-2\n",
    "        n2 = j-1\n",
    "        n3 = j+1\n",
    "        n4 = j+2\n",
    "        value = abs((sorted_list[n1]+sorted_list[n2]+sorted_list[n3]+sorted_list[n4] \n",
    "                                  - 2*k*sorted_list[j]) / (2*k))\n",
    "    elif j == 0:\n",
    "        n1 = 1\n",
    "        n2 = 2\n",
    "        n3 = 3\n",
    "        n4 = 4\n",
    "        value = abs((sorted_list[n1]+sorted_list[n2]+sorted_list[n3]+sorted_list[n4] \n",
    "                                  - 2*k*sorted_list[0]) / (2*k))\n",
    "    elif j == 1:\n",
    "        n1 = 0\n",
    "        n2 = 2\n",
    "        n3 = 3\n",
    "        n4 = 4\n",
    "        value = abs((sorted_list[n1]+sorted_list[n2]+sorted_list[n3]+sorted_list[n4] \n",
    "                                  - 2*k*sorted_list[1]) / (2*k))\n",
    "    elif j == length-1:\n",
    "        n1 = length-2\n",
    "        n2 = length-3\n",
    "        n3 = length-4\n",
    "        n4 = length-5\n",
    "        value = abs((sorted_list[n1]+sorted_list[n2]+sorted_list[n3]+sorted_list[n4] \n",
    "                                  - 2*k*sorted_list[length-1]) / (2*k))\n",
    "    else:\n",
    "        n1 = length-1\n",
    "        n2 = length-3\n",
    "        n3 = length-4\n",
    "        n4 = length-5\n",
    "        value = abs((sorted_list[n1]+sorted_list[n2]+sorted_list[n3]+sorted_list[n4] \n",
    "                                  - 2*k*sorted_list[length-2]) / (2*k))\n",
    "        \n",
    "    if value == 0.0:\n",
    "        return 1/0.0000001, [n1, n2, n3, n4]\n",
    "    else:\n",
    "        return 1/value, [n1, n2, n3, n4]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "719daad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applies density-based anomaly detection to a specific feature - filters all data points out\n",
    "#that have an average density of less than a specified threshold\n",
    "def filter_density(data, examined, value, threshold):\n",
    "    \n",
    "    feature = data[examined]\n",
    "    length = len(feature)\n",
    "    \n",
    "    new_feature0 = []\n",
    "    new_feature1 = []\n",
    "    new_feature2 = []\n",
    "    new_feature3 = []\n",
    "    new_feature4 = []\n",
    "    new_feature5 = []\n",
    "    new_feature6 = []\n",
    "    \n",
    "    remember = np.argsort(feature).tolist()\n",
    "    sorted_list = sorted(feature)\n",
    "    k = 2\n",
    "    \n",
    "    variance = np.var(feature)\n",
    "        \n",
    "    for i in range(length):\n",
    "        j = remember.index(i)\n",
    "\n",
    "        avg_density, _ = average_density(j, length, sorted_list, k)\n",
    "\n",
    "        if 1/avg_density < threshold*variance:\n",
    "            new_feature0.append(data[0][i])\n",
    "            new_feature1.append(data[1][i])\n",
    "            new_feature2.append(data[2][i])\n",
    "            new_feature3.append(data[3][i])\n",
    "            new_feature4.append(data[4][i])\n",
    "            new_feature5.append(data[5][i])\n",
    "            new_feature6.append(data[6][i])\n",
    "\n",
    "    new_features = [new_feature0, new_feature1, new_feature2, new_feature3, new_feature4, new_feature5, new_feature6]\n",
    "    new_labels = [value]*len(new_feature0)\n",
    "    \n",
    "    return new_features, new_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcc69468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applies density-based anomaly detection to a specific feature - filters all data points out\n",
    "#based on the relative density of itself to its neighbours\n",
    "def filter_relative_density(data, examined, value, threshold):\n",
    "    \n",
    "    feature = data[examined]\n",
    "    length = len(feature)\n",
    "    \n",
    "    new_feature0 = []\n",
    "    new_feature1 = []\n",
    "    new_feature2 = []\n",
    "    new_feature3 = []\n",
    "    new_feature4 = []\n",
    "    new_feature5 = []\n",
    "    new_feature6 = []\n",
    "    \n",
    "    remember = np.argsort(feature).tolist()\n",
    "    sorted_list = sorted(feature)\n",
    "    k = 2\n",
    "        \n",
    "    for i in range(length):\n",
    "        j = remember.index(i)\n",
    "\n",
    "        #Takes the 2 values above and two values below in the sorted list, and sum their distances from the current feature\n",
    "        avg_density, neighbours = average_density(j, length, sorted_list, k)\n",
    "        \n",
    "        #relative to the density of those neighbours\n",
    "        sum_neigh_dens = 0\n",
    "        for each in neighbours:\n",
    "            dens, _ = average_density(each, length, sorted_list, k)\n",
    "            sum_neigh_dens += dens\n",
    "        sum_neigh_dens /= 4\n",
    "        relative_density = avg_density / sum_neigh_dens\n",
    "\n",
    "        #LOF\n",
    "        if relative_density == 0.0:\n",
    "            lof = 1/0.0000001\n",
    "        else:\n",
    "            lof = 1/relative_density\n",
    "          \n",
    "        if lof <= 1*threshold:\n",
    "            new_feature0.append(data[0][i])\n",
    "            new_feature1.append(data[1][i])\n",
    "            new_feature2.append(data[2][i])\n",
    "            new_feature3.append(data[3][i])\n",
    "            new_feature4.append(data[4][i])\n",
    "            new_feature5.append(data[5][i])\n",
    "            new_feature6.append(data[6][i])\n",
    "\n",
    "    new_features = [new_feature0, new_feature1, new_feature2, new_feature3, new_feature4, new_feature5, new_feature6]\n",
    "    new_labels = [value]*len(new_feature0)\n",
    "    \n",
    "    return new_features, new_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3704bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applies statistical-based anomaly detection to a specific feature - filters all data points out\n",
    "#based on the interquartile range\n",
    "def filter_feature_IQR(data, examined, value, threshold):\n",
    "    \n",
    "    feature = data[examined]\n",
    "    \n",
    "    Q1 = np.percentile(feature, 25)\n",
    "    Q3 = np.percentile(feature, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    new_feature0 = []\n",
    "    new_feature1 = []\n",
    "    new_feature2 = []\n",
    "    new_feature3 = []\n",
    "    new_feature4 = []\n",
    "    new_feature5 = []\n",
    "    new_feature6 = []\n",
    "\n",
    "    for i in range(len(feature)):\n",
    "        if feature[i] <= Q3 + 1.5*IQR and feature[i] >= Q1 - 1.5*IQR:\n",
    "            new_feature0.append(data[0][i])\n",
    "            new_feature1.append(data[1][i])\n",
    "            new_feature2.append(data[2][i])\n",
    "            new_feature3.append(data[3][i])\n",
    "            new_feature4.append(data[4][i])\n",
    "            new_feature5.append(data[5][i])\n",
    "            new_feature6.append(data[6][i])\n",
    "    \n",
    "    new_features = [new_feature0, new_feature1, new_feature2, new_feature3, new_feature4, new_feature5, new_feature6]\n",
    "    \n",
    "    return new_features, [value]*len(new_feature0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fdd8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applies statistical-based anomaly detection to a specific feature - filters all data points out\n",
    "#based on the 3 sigma rule\n",
    "def filter_features(data, examined, value, threshold):\n",
    "    \n",
    "    feature = data[examined]\n",
    "    \n",
    "    mu = np.mean(feature)\n",
    "    variance = np.var(feature)\n",
    "    sigma = math.sqrt(variance)\n",
    "    #sigma = np.inf #toggles off filtering\n",
    "    \n",
    "    new_feature0 = []\n",
    "    new_feature1 = []\n",
    "    new_feature2 = []\n",
    "    new_feature3 = []\n",
    "    new_feature4 = []\n",
    "    new_feature5 = []\n",
    "    new_feature6 = []\n",
    "    \n",
    "    for i in range(len(feature)):\n",
    "        if feature[i] <= mu+3*sigma and feature[i] >= mu-3*sigma:\n",
    "            new_feature0.append(data[0][i])\n",
    "            new_feature1.append(data[1][i])\n",
    "            new_feature2.append(data[2][i])\n",
    "            new_feature3.append(data[3][i])\n",
    "            new_feature4.append(data[4][i])\n",
    "            new_feature5.append(data[5][i])\n",
    "            new_feature6.append(data[6][i])\n",
    "    \n",
    "    new_features = [new_feature0, new_feature1, new_feature2, new_feature3, new_feature4, new_feature5, new_feature6]\n",
    "    \n",
    "    return new_features, [value]*len(new_feature0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b14e0f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest training non filtered data\n",
    "examine_adv = extract(advTrain[0:split])\n",
    "examine_benign = extract(newtrainbenign[0:split1])\n",
    "examine_AGD = extract(newtrainAGD[0:split2])\n",
    "combined = [0]*7\n",
    "combined[0] = examine_adv[0]+examine_benign[0]+examine_AGD[0]\n",
    "combined[1] = examine_adv[1]+examine_benign[1]+examine_AGD[1]\n",
    "combined[2] = examine_adv[2]+examine_benign[2]+examine_AGD[2]\n",
    "combined[3] = examine_adv[3]+examine_benign[3]+examine_AGD[3]\n",
    "combined[4] = examine_adv[4]+examine_benign[4]+examine_AGD[4]\n",
    "combined[5] = examine_adv[5]+examine_benign[5]+examine_AGD[5]\n",
    "combined[6] = examine_adv[6]+examine_benign[6]+examine_AGD[6]\n",
    "combined_labels = ['2']*20000+['0']*20000+['1']*20000\n",
    "\n",
    "#comment out above to use filtered data\n",
    "#Random Forests Training on filtered data\n",
    "X = convert(combined)\n",
    "y = combined_labels\n",
    "clf = RandomForestClassifier(max_depth=7, random_state=0)\n",
    "clf.fit(X, y)\n",
    "\n",
    "#Random Forests Predicting\n",
    "X9 = convert(extract(advTest50[0:split]+newtestbenign[0:split1]+newtestAGD[0:split2]))\n",
    "mixedlabels = advTestlabel50[0:split]+newtestbenignlabel[0:split1]+newtestAGDlabel[0:split2]\n",
    "X6 = convert(extract(advTest50[0:10000]))               #MaskDGA data - completely ADV\n",
    "X7 = convert(extract(newtestbenign[0:10000]))           #original data - completely benign\n",
    "X8 = convert(extract(newtestAGD[0:10000]))              #original data - completely AGD\n",
    "predictions6 = clf.predict(X6)\n",
    "predictions7 = clf.predict(X7)\n",
    "predictions8 = clf.predict(X8)\n",
    "predictions9 = clf.predict(X9)\n",
    "\n",
    "# print(f1_score(advTestlabel50[0:10000], predictions6, average='micro'))           #test_sub-cnn_prec-50\n",
    "# print(f1_score(newtestbenignlabel[0:10000], predictions7, average='micro'))\n",
    "# print(f1_score(newtestAGDlabel[0:10000], predictions8, average='micro'))\n",
    "# print(f1_score(mixedlabels, predictions9, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c55c38d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selects the feature that filters out the most anomalies based on the\n",
    "#anomaly detection method used\n",
    "def auto_select(data, string, thresholds):\n",
    "    \n",
    "    #comment out this line too apply filtering\n",
    "    return [[data, [string]*len(data[0])]]         #no filtering\n",
    "    \n",
    "    output = []\n",
    "\n",
    "    for each in thresholds:\n",
    "        value = 20000\n",
    "        for ind in [1,2,3,6]:\n",
    "            #select anomaly detection method to use\n",
    "            result, label = filter_feature_IQR(data, ind, string, each)\n",
    "            #result, label = filter_features(data, ind, string, each)\n",
    "            #result, label = filter_density(data, ind, string, each)\n",
    "            #result, label = filter_relative_density(data, ind, string, each)\n",
    "            if len(label) <= value:\n",
    "                value = len(label)\n",
    "                best_result = result\n",
    "                best_label = label\n",
    "        \n",
    "        output.append([best_result, best_label])   #best feature for that threshold\n",
    "                \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c2fd817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See how varying threshold for density filtering method affects classification\n",
    "split=split1=split2= 20000\n",
    "thresholds = [0.001,0.01,0.1,1,10,100]             #for density\n",
    "#thresholds = [1,10,100,1000,10000]                #for relative density\n",
    "\n",
    "score2 = []\n",
    "score3 = []\n",
    "score4 = []\n",
    "score5 = []\n",
    "X6 = convert(extract(advTest50[0:10000]))               #MaskDGA data - completely ADV\n",
    "X7 = convert(extract(newtestbenign[0:10000]))           #original data - completely benign\n",
    "X8 = convert(extract(newtestAGD[0:10000]))              #original data - completely AGD\n",
    "X9 = convert(extract(advTest50[0:split]+newtestbenign[0:split1]+newtestAGD[0:split2]))\n",
    "mixedlabels = advTestlabel50[0:split]+newtestbenignlabel[0:split1]+newtestAGDlabel[0:split2]\n",
    "\n",
    "#extract features\n",
    "examine_adv = extract(advTrain[0:split])\n",
    "examine_benign = extract(newtrainbenign[0:split1])\n",
    "examine_AGD = extract(newtrainAGD[0:split2])\n",
    "\n",
    "#filter anomalies\n",
    "adv_output = auto_select(examine_adv, '2', thresholds)\n",
    "benign_output = auto_select(examine_benign, '0', thresholds)\n",
    "AGD_output = auto_select(examine_AGD, '1', thresholds)\n",
    "\n",
    "for i in range(len(thresholds)):\n",
    "    \n",
    "    #convert form\n",
    "    combined = [0]*7\n",
    "    combined[0] = adv_output[i][0][0]+benign_output[i][0][0]+AGD_output[i][0][0]\n",
    "    combined[1] = adv_output[i][0][1]+benign_output[i][0][1]+AGD_output[i][0][1]\n",
    "    combined[2] = adv_output[i][0][2]+benign_output[i][0][2]+AGD_output[i][0][2]\n",
    "    combined[3] = adv_output[i][0][3]+benign_output[i][0][3]+AGD_output[i][0][3]\n",
    "    combined[4] = adv_output[i][0][4]+benign_output[i][0][4]+AGD_output[i][0][4]\n",
    "    combined[5] = adv_output[i][0][5]+benign_output[i][0][5]+AGD_output[i][0][5]\n",
    "    combined[6] = adv_output[i][0][6]+benign_output[i][0][6]+AGD_output[i][0][6]\n",
    "    combined_labels = adv_output[i][1] + benign_output[i][1] + AGD_output[i][1]\n",
    "    print(len(combined_labels))\n",
    "\n",
    "    clf = RandomForestClassifier(max_depth=7, random_state=0)\n",
    "    clf.fit(convert(combined), combined_labels)\n",
    "    \n",
    "    #predict and record f1 scores\n",
    "    score2.append(round(f1_score(advTestlabel50[0:10000], clf.predict(X6), average='micro'),3))\n",
    "    score3.append(round(f1_score(newtestbenignlabel[0:10000], clf.predict(X7), average='micro'),3))\n",
    "    score4.append(round(f1_score(newtestAGDlabel[0:10000], clf.predict(X8), average='micro'),3))\n",
    "    score5.append(round(f1_score(mixedlabels, clf.predict(X9), average='micro'),3))\n",
    "\n",
    "# print(score2)\n",
    "# print(score3)\n",
    "# print(score4)\n",
    "# print(score5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed01c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this only after running the filtering by density method\n",
    "plt.plot([math.log(i,10) for i in thresholds], score2, label='ADV')\n",
    "plt.plot([math.log(i,10) for i in thresholds], score3, label='normal')\n",
    "plt.plot([math.log(i,10) for i in thresholds], score4, label='AGD')\n",
    "plt.plot([math.log(i,10) for i in thresholds], score5, label='mixed')\n",
    "plt.xlabel('threshold (log10) (*var(feature))')\n",
    "plt.ylabel('f1 score') \n",
    "plt.title('Density: Performance of Different Thresholds on Test Sets')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a68c157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Run this only after running the filtering by relative density method\n",
    "# plt.plot([math.log(i,10) for i in thresholds], score2, label='ADV')\n",
    "# plt.plot([math.log(i,10) for i in thresholds], score3, label='normal')\n",
    "# plt.plot([math.log(i,10) for i in thresholds], score4, label='AGD')\n",
    "# plt.plot([math.log(i,10) for i in thresholds], score5, label='mixed')\n",
    "# plt.xlabel('threshold (log10)')\n",
    "# plt.ylabel('f1 score') \n",
    "# plt.title('Relative Density: Performance of Different Thresholds on Test Sets')\n",
    "# plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21bd8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging Ensemble Learning\n",
    "#MAKE SURE TO UNCOMMENT THE FILTERING METHOD AT THE BOTTOM TO STORE RESULTS BEFORE RUNNING\n",
    "split=split1=split2= 20000\n",
    "#thresholds = [0.01]           #for density\n",
    "thresholds = [10]             #for relative density\n",
    "score2 = []\n",
    "score3 = []\n",
    "score4 = []\n",
    "score5 = []\n",
    "X6 = convert(extract(advTest50[0:10000]))               #MaskDGA data - completely ADV\n",
    "X7 = convert(extract(newtestbenign[0:10000]))           #original data - completely benign\n",
    "X8 = convert(extract(newtestAGD[0:10000]))              #original data - completely AGD\n",
    "X9 = convert(extract(advTest50[0:split]+newtestbenign[0:split1]+newtestAGD[0:split2]))\n",
    "mixedlabels = advTestlabel50[0:split]+newtestbenignlabel[0:split1]+newtestAGDlabel[0:split2]\n",
    "\n",
    "for i in range(10):\n",
    "    #sample with replacement\n",
    "    adv_sample = choices(advTrain, k=split)\n",
    "    benign_sample = choices(newtrainbenign, k=split1)\n",
    "    AGD_sample = choices(newtrainAGD, k=split2)\n",
    "    \n",
    "    #extract features\n",
    "    examine_adv = extract(adv_sample)\n",
    "    examine_benign = extract(benign_sample)\n",
    "    examine_AGD = extract(AGD_sample)\n",
    "\n",
    "    #filter anomalies\n",
    "    adv_output = auto_select(examine_adv, '2', thresholds)\n",
    "    benign_output = auto_select(examine_benign, '0', thresholds)\n",
    "    AGD_output = auto_select(examine_AGD, '1', thresholds)\n",
    "\n",
    "    #convert form\n",
    "    combined = [0]*7\n",
    "    combined[0] = adv_output[0][0][0]+benign_output[0][0][0]+AGD_output[0][0][0]\n",
    "    combined[1] = adv_output[0][0][1]+benign_output[0][0][1]+AGD_output[0][0][1]\n",
    "    combined[2] = adv_output[0][0][2]+benign_output[0][0][2]+AGD_output[0][0][2]\n",
    "    combined[3] = adv_output[0][0][3]+benign_output[0][0][3]+AGD_output[0][0][3]\n",
    "    combined[4] = adv_output[0][0][4]+benign_output[0][0][4]+AGD_output[0][0][4]\n",
    "    combined[5] = adv_output[0][0][5]+benign_output[0][0][5]+AGD_output[0][0][5]\n",
    "    combined[6] = adv_output[0][0][6]+benign_output[0][0][6]+AGD_output[0][0][6]\n",
    "    combined_labels = adv_output[0][1] + benign_output[0][1] + AGD_output[0][1]\n",
    "    print(len(combined_labels))\n",
    "\n",
    "    #training each model on a different sample of the same training set\n",
    "    clf = RandomForestClassifier(max_depth=7, random_state=0)\n",
    "    clf.fit(convert(combined), combined_labels)\n",
    "    \n",
    "    #predict and record f1 scores\n",
    "    score2.append(round(f1_score(advTestlabel50[0:10000], clf.predict(X6), average='micro'),3))\n",
    "    score3.append(round(f1_score(newtestbenignlabel[0:10000], clf.predict(X7), average='micro'),3))\n",
    "    score4.append(round(f1_score(newtestAGDlabel[0:10000], clf.predict(X8), average='micro'),3))\n",
    "    score5.append(round(f1_score(mixedlabels, clf.predict(X9), average='micro'),3))\n",
    "\n",
    "#MAKE SURE TO UNCOMMENT THE FILTERING METHOD TO STORE RESULTS BEFORE RUNNING\n",
    "#bagging_base = [score2,score3,score4,score5]        #no filtering\n",
    "#bagging_ada1 = [score2,score3,score4,score5]        #IQR\n",
    "#bagging_ada2 = [score2,score3,score4,score5]        #SD\n",
    "#bagging_ada3 = [score2,score3,score4,score5]        #density\n",
    "#bagging_ada4 = [score2,score3,score4,score5]        #relative density\n",
    "\n",
    "# print(score2)\n",
    "# print(score3)\n",
    "# print(score4)\n",
    "# print(score5)\n",
    "# print(round(np.mean(score2),3))\n",
    "# print(round(np.mean(score3),3))\n",
    "# print(round(np.mean(score4),3))\n",
    "# print(round(np.mean(score5),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b48bfde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacking Ensemble Learning\n",
    "#MAKE SURE TO UNCOMMENT THE FILTERING METHOD AT THE BOTTOM TO STORE RESULTS BEFORE RUNNING\n",
    "#get a stacking ensemble of models\n",
    "level0 = list()\n",
    "level0.append(('RF', RandomForestClassifier(max_depth=7, random_state=0)))\n",
    "level0.append(('cart', tree.DecisionTreeClassifier()))\n",
    "level0.append(('svm', svm.SVC()))\n",
    "level0.append(('bayes', GaussianNB()))\n",
    "level1 = LogisticRegression()\n",
    "model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "\n",
    "split=split1=split2= 20000\n",
    "thresholds = [0.01]           #for density\n",
    "#thresholds = [10]             #for relative density\n",
    "score6 = []\n",
    "score7 = []\n",
    "score8 = []\n",
    "score9 = []\n",
    "X6 = convert(extract(advTest50[0:10000]))               #MaskDGA data - completely ADV\n",
    "X7 = convert(extract(newtestbenign[0:10000]))           #original data - completely benign\n",
    "X8 = convert(extract(newtestAGD[0:10000]))              #original data - completely AGD\n",
    "X9 = convert(extract(advTest50[0:split]+newtestbenign[0:split1]+newtestAGD[0:split2]))\n",
    "mixedlabels = advTestlabel50[0:split]+newtestbenignlabel[0:split1]+newtestAGDlabel[0:split2]\n",
    "\n",
    "for i in range(5):\n",
    "    #sample with replacement\n",
    "    adv_sample = choices(advTrain, k=split)\n",
    "    benign_sample = choices(newtrainbenign, k=split1)\n",
    "    AGD_sample = choices(newtrainAGD, k=split2)\n",
    "    \n",
    "    #extract features\n",
    "    examine_adv = extract(adv_sample)\n",
    "    examine_benign = extract(benign_sample)\n",
    "    examine_AGD = extract(AGD_sample)\n",
    "\n",
    "    #filter anomalies\n",
    "    adv_output = auto_select(examine_adv, '2', thresholds)\n",
    "    benign_output = auto_select(examine_benign, '0', thresholds)\n",
    "    AGD_output = auto_select(examine_AGD, '1', thresholds)\n",
    "\n",
    "    #convert form\n",
    "    combined = [0]*7\n",
    "    combined[0] = adv_output[0][0][0]+benign_output[0][0][0]+AGD_output[0][0][0]\n",
    "    combined[1] = adv_output[0][0][1]+benign_output[0][0][1]+AGD_output[0][0][1]\n",
    "    combined[2] = adv_output[0][0][2]+benign_output[0][0][2]+AGD_output[0][0][2]\n",
    "    combined[3] = adv_output[0][0][3]+benign_output[0][0][3]+AGD_output[0][0][3]\n",
    "    combined[4] = adv_output[0][0][4]+benign_output[0][0][4]+AGD_output[0][0][4]\n",
    "    combined[5] = adv_output[0][0][5]+benign_output[0][0][5]+AGD_output[0][0][5]\n",
    "    combined[6] = adv_output[0][0][6]+benign_output[0][0][6]+AGD_output[0][0][6]\n",
    "    combined_labels = adv_output[0][1] + benign_output[0][1] + AGD_output[0][1]\n",
    "    print(len(combined_labels))\n",
    "\n",
    "    #training each model on a different sample of the same training set\n",
    "    clf = model\n",
    "    clf.fit(convert(combined), combined_labels)\n",
    "    \n",
    "    #predict and record f1 scores\n",
    "    score6.append(round(f1_score(advTestlabel50[0:10000], clf.predict(X6), average='micro'),3))\n",
    "    score7.append(round(f1_score(newtestbenignlabel[0:10000], clf.predict(X7), average='micro'),3))\n",
    "    score8.append(round(f1_score(newtestAGDlabel[0:10000], clf.predict(X8), average='micro'),3))\n",
    "    score9.append(round(f1_score(mixedlabels, clf.predict(X9), average='micro'),3))\n",
    "\n",
    "#MAKE SURE TO UNCOMMENT THE FILTERING METHOD TO STORE RESULTS BEFORE RUNNING\n",
    "#stacking_base = [score6,score7,score8,score9]        #no filtering\n",
    "#stacking_ada1 = [score6,score7,score8,score9]        #IQR\n",
    "#stacking_ada2 = [score6,score7,score8,score9]        #SD\n",
    "#stacking_ada3 = [score6,score7,score8,score9]        #density\n",
    "#stacking_ada4 = [score6,score7,score8,score9]        #relative density\n",
    "\n",
    "# print(score6)\n",
    "# print(score7)\n",
    "# print(score8)\n",
    "# print(score9)\n",
    "# print(round(np.mean(score6),3))\n",
    "# print(round(np.mean(score7),3))\n",
    "# print(round(np.mean(score8),3))\n",
    "# print(round(np.mean(score9),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ee3dacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compares bagging and stacking for the base results (no filtering)\n",
    "#computes the means and standard deviation of each set for bagging\n",
    "#computes the means of each set for stacking\n",
    "bagging_means = []\n",
    "bagging_std = []\n",
    "stacking_means = []\n",
    "\n",
    "for each in bagging_base:\n",
    "    bagging_means.append(np.mean(each))\n",
    "    bagging_std.append(np.std(each))\n",
    "\n",
    "for each in stacking_base:\n",
    "    stacking_means.append(np.mean(each))\n",
    "\n",
    "upper = []\n",
    "lower = []\n",
    "for i in range(len(bagging_means)):\n",
    "    upper.append(bagging_means[i]+1.96*bagging_std[i])\n",
    "    lower.append(bagging_means[i]-1.96*bagging_std[i])\n",
    "    \n",
    "#plots the 95% confidence bands of the bagging resuts and compares with the stacking results\n",
    "plt.plot(['ADV','normal','AGD','mixed'],bagging_means,'o', label='bagging')\n",
    "plt.plot(['ADV','ADV','ADV','ADV'],[lower[0],lower[0],bagging_means[0],upper[0]])\n",
    "plt.plot(['normal','normal','normal','normal'],[lower[1],lower[1],bagging_means[1],upper[1]])\n",
    "plt.plot(['AGD','AGD','AGD','AGD'],[lower[2],lower[2],bagging_means[2],upper[2]])\n",
    "plt.plot(['mixed','mixed','mixed','mixed'],[lower[3],lower[3],bagging_means[3],upper[3]])\n",
    "plt.plot(['ADV','normal','AGD','mixed'],stacking_means,'x', label='stacking')\n",
    "plt.legend()\n",
    "plt.title('Base Performances of Ensemble Learning on Four Test Sets')\n",
    "plt.xlabel('test set')\n",
    "plt.ylabel('f1 score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "eba9ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot and compare ensembling techniques to anomaly detection methods\n",
    "final_scores2 = []\n",
    "final_scores3 = []\n",
    "final_scores4 = []\n",
    "final_scores5 = []\n",
    "\n",
    "final_scores2.append(np.mean(bagging_base[0]))\n",
    "final_scores2.append(np.mean(bagging_ada1[0]))\n",
    "final_scores2.append(np.mean(bagging_ada2[0]))\n",
    "final_scores2.append(np.mean(bagging_ada3[0]))\n",
    "final_scores2.append(np.mean(bagging_ada4[0]))\n",
    "final_scores2.append(np.mean(stacking_base[0]))\n",
    "final_scores2.append(np.mean(stacking_ada1[0]))\n",
    "final_scores2.append(np.mean(stacking_ada2[0]))\n",
    "final_scores2.append(np.mean(stacking_ada3[0]))\n",
    "final_scores2.append(np.mean(stacking_ada4[0]))\n",
    "\n",
    "final_scores3.append(np.mean(bagging_base[1]))\n",
    "final_scores3.append(np.mean(bagging_ada1[1]))\n",
    "final_scores3.append(np.mean(bagging_ada2[1]))\n",
    "final_scores3.append(np.mean(bagging_ada3[1]))\n",
    "final_scores3.append(np.mean(bagging_ada4[1]))\n",
    "final_scores3.append(np.mean(stacking_base[1]))\n",
    "final_scores3.append(np.mean(stacking_ada1[1]))\n",
    "final_scores3.append(np.mean(stacking_ada2[1]))\n",
    "final_scores3.append(np.mean(stacking_ada3[1]))\n",
    "final_scores3.append(np.mean(stacking_ada4[1]))\n",
    "\n",
    "final_scores4.append(np.mean(bagging_base[2]))\n",
    "final_scores4.append(np.mean(bagging_ada1[2]))\n",
    "final_scores4.append(np.mean(bagging_ada2[2]))\n",
    "final_scores4.append(np.mean(bagging_ada3[2]))\n",
    "final_scores4.append(np.mean(bagging_ada4[2]))\n",
    "final_scores4.append(np.mean(stacking_base[2]))\n",
    "final_scores4.append(np.mean(stacking_ada1[2]))\n",
    "final_scores4.append(np.mean(stacking_ada2[2]))\n",
    "final_scores4.append(np.mean(stacking_ada3[2]))\n",
    "final_scores4.append(np.mean(stacking_ada4[2]))\n",
    "\n",
    "final_scores5.append(np.mean(bagging_base[3]))\n",
    "final_scores5.append(np.mean(bagging_ada1[3]))\n",
    "final_scores5.append(np.mean(bagging_ada2[3]))\n",
    "final_scores5.append(np.mean(bagging_ada3[3]))\n",
    "final_scores5.append(np.mean(bagging_ada4[3]))\n",
    "final_scores5.append(np.mean(stacking_base[3]))\n",
    "final_scores5.append(np.mean(stacking_ada1[3]))\n",
    "final_scores5.append(np.mean(stacking_ada2[3]))\n",
    "final_scores5.append(np.mean(stacking_ada3[3]))\n",
    "final_scores5.append(np.mean(stacking_ada4[3]))\n",
    "\n",
    "ensemble_labels = ['bag none','bag IQR','bag SD','bag dens','bag rel dens',\n",
    "                   'stack none','stack IQR','stack SD','stack dens','stack rel dens']\n",
    "plt.figure(figsize =(12, 5))\n",
    "plt.plot(ensemble_labels,final_scores2,'-o',label='ADV')\n",
    "plt.plot(ensemble_labels,final_scores3,'-o',label='normal')\n",
    "plt.plot(ensemble_labels,final_scores4,'-o',label='AGD')\n",
    "plt.plot(ensemble_labels,final_scores5,'-o',label='mixed')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.title('Performances Post Filtering Anomalies on Four Test Sets')\n",
    "plt.xlabel('anomaly detection method and ensemble technique')\n",
    "plt.ylabel('f1 score')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
